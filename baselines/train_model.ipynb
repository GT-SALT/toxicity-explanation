{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNqkf0nRbEKzdlh/bWGebFW"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"iK0GvZ41Dp5W"},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","# mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# cd into project directory\n","%cd /content/drive/My\\ Drive/Georgia_Tech/Spring_2021/sbic_stereotypes/baselines\n","\n","# Useful Constants\n","DATA_DIR = '../data/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"afHTYEim6IkR"},"source":["!pip install transformers\n","!pip install datasets\n","\n","import torch\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yWJslDxJ5tkq"},"source":["## Set all parameters here ##\n","gpt_dict_5epoch = {\n","                    'MODEL_NAME':'openai-gpt', \\\n","                    'OUTPUT_DIR':'model/gpt_5epoch', \\\n","                    'LEARNING_RATE': 5e-6, \\\n","                    'SAVE_STEPS': 22367, \\\n","                    'EPOCHS': 5.0\n","                  }\n","\n","gpt2_dict_5epoch =  {\n","                      'MODEL_NAME':'gpt2', \\\n","                      'OUTPUT_DIR':'model/gpt2_5epoch', \\\n","                      'LEARNING_RATE': 1e-5, \\\n","                      'SAVE_STEPS': 24075, \\\n","                      'EPOCHS': 5.0\n","                    }\n","\n","active_dict = gpt_dict_5epoch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r3gHdyvM6N6O"},"source":["from data_preprocessing import *\n","\n","# Read and Clean Data\n","from_train_file = DATA_DIR + 'SBIC.v2.trn.csv'\n","from_dev_file = DATA_DIR + 'SBIC.v2.dev.csv'\n","to_train_file = 'data/baseline_train_text.csv'\n","to_dev_file = 'data/baseline_dev_text.csv'\n","\n","clean_df(from_train_file, to_train_file)\n","clean_df(from_dev_file, to_dev_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIOp0-HLHG2v"},"source":["from datasets import load_dataset\n","\n","datasets = load_dataset(\"csv\", data_files={\"train\": to_train_file, \"validation\": to_dev_file})\n","print(datasets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTehhAB6AIe_"},"source":["# We need to create the model and tokenizer\n","tokenizer = setup_tokenizer(active_dict['MODEL_NAME'])\n","tokenized_datasets = datasets.map(lambda examples: tokenizer(examples[\"text\"]), \\\n","                                    batched=True, num_proc=4, \\\n","                                    remove_columns=[\"text\"])\n","\n","print(tokenized_datasets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nFkTwm-Nl85H"},"source":["# Normalize the text length as Blocks\n","block_size = 128\n","\n","def group_texts(examples):\n","    # Concatenate all texts.\n","    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    \n","    if total_length % block_size:\n","      remainder = block_size - (total_length % block_size)\n","    else:\n","      remainder = 0\n","    \n","    pad_extension = [tokenizer.pad_token_id for _ in range(remainder)]\n","    attention_extension = [0 for _ in range(remainder)]\n","\n","    concatenated_examples['input_ids'].extend(pad_extension)\n","    concatenated_examples['attention_mask'].extend(attention_extension)\n","    total_length += remainder\n","    \n","    # Split by chunks of max_len.\n","    result = {\n","        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n","        for k, t in concatenated_examples.items()\n","    }\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result\n","\n","lm_datasets = tokenized_datasets.map(\n","    group_texts,\n","    batched=True,\n","    batch_size=1000,\n","    num_proc=4,\n",")\n","\n","print(lm_datasets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OEpn2adTGNOC"},"source":["# Load PreTrained Model and train it\n","from transformers import AutoModelForCausalLM\n","from transformers import Trainer, TrainingArguments\n","\n","model = AutoModelForCausalLM.from_pretrained(active_dict['MODEL_NAME'])\n","model.resize_token_embeddings(len(tokenizer))\n","model.train()\n","\n","training_args = TrainingArguments(\n","    output_dir = active_dict['OUTPUT_DIR'],\n","    evaluation_strategy = 'steps',\n","    eval_steps = 2000,\n","    save_steps = active_dict['SAVE_STEPS'],\n","    save_total_limit = 1,\n","    warmup_steps = 5000,\n","    learning_rate = active_dict['LEARNING_RATE'],\n","    per_device_train_batch_size = 4,\n","    num_train_epochs = active_dict['EPOCHS'],\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=lm_datasets[\"train\"],\n","    eval_dataset=lm_datasets[\"validation\"],\n",")\n","\n","trainer.train()"],"execution_count":null,"outputs":[]}]}