{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_classifier.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNbkACDYspuPczeyLPZfOn9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"zDT3TOYbdGS_"},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","# mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# cd into project directory\n","%cd /content/drive/My\\ Drive/Georgia_Tech/Spring_2021/sbic_stereotypes/src/classification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"23L2roQ1dvLZ"},"source":["!pip install transformers\n","!pip install datasets\n","\n","import torch\n","import pandas as pd\n","import numpy as np\n","\n","# Useful constants\n","DATA_DIR = '../../data/'\n","MODEL_NAME = 'bert-base-uncased'\n","CLASSIFY_COL = 'whoTarget'\n","\n","OFFENSE_PARAMS = {\n","      'clean_data_file': 'data/train_' + CLASSIFY_COL + '.csv',\n","      'model_output': 'model/' + CLASSIFY_COL,\n","      'max_length': 128,\n","      'lr': 5e-6,\n","      'batch_size': 32,\n","      'num_epochs': 2.0,\n","  }\n","\n","LEWD_PARAMS = {\n","      'clean_data_file': 'data/train_' + CLASSIFY_COL + '.csv',\n","      'model_output': 'model/' + CLASSIFY_COL,\n","      'max_length': 128,\n","      'lr': 5e-6,\n","      'batch_size': 32,\n","      'num_epochs': 1.0,\n","  }\n","\n","INTENT_PARAMS = {\n","      'clean_data_file': 'data/train_' + CLASSIFY_COL + '.csv',\n","      'model_output': 'model/' + CLASSIFY_COL,\n","      'max_length': 128,\n","      'lr': 5e-7,\n","      'batch_size': 32,\n","      'num_epochs': 1.0,\n","  }\n","\n","GROUP_PARAMS = {\n","      'clean_data_file': 'data/train_' + CLASSIFY_COL + '.csv',\n","      'model_output': 'model/' + CLASSIFY_COL,\n","      'max_length': 128,\n","      'lr': 5e-6,\n","      'batch_size': 32,\n","      'num_epochs': 2.0,\n","  }\n","\n","PARAMS = GROUP_PARAMS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxdw1J4mhNv6"},"source":["from classifier_utils import *\n","\n","# Classify column\n","df = pd.read_csv(DATA_DIR + 'SBIC.v2.trn.csv')\n","df = prep_df_for_classification(df, PARAMS['clean_data_file'], CLASSIFY_COL)\n","print(df[CLASSIFY_COL].isna().any())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qIzArseLrUiu"},"source":["from transformers import BertTokenizer\n","from datasets import Dataset\n","import statistics as stats\n","\n","dataset = Dataset.from_pandas(df)\n","datasets = dataset.train_test_split(test_size=0.2, shuffle=True)\n","\n","tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n","# This takes a while to run\n","# del tokenized_datasets\n","if 'tokenized_datasets' not in globals():\n","  tokenized_datasets = datasets.map(lambda row: tokenizer(row[\"post\"], \\\n","                                                          truncation=True, \\\n","                                                          padding='max_length', \\\n","                                                          max_length=PARAMS['max_length']), \\\n","                                      batched=True, num_proc=4, \\\n","                                      remove_columns=[\"post\"])\n","\n","print(tokenized_datasets)\n","\n","## Compute Statistics in case we are interested.\n","compute_statistics(tokenized_datasets['train']['input_ids'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYw1ujsC4HkK"},"source":["# Prepare Final Dataset for Training\n","lm_datasets = {}\n","\n","for key in tokenized_datasets.keys():\n","  lm_datasets[key] = tokenized_datasets[key].rename_column(CLASSIFY_COL, 'labels')\n","\n","print(lm_datasets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qJcobPkFTdMq"},"source":["from transformers import BertForSequenceClassification\n","from transformers import Trainer, TrainingArguments\n","import math\n","\n","model = BertForSequenceClassification.from_pretrained(MODEL_NAME)\n","model.train()\n","\n","num_rows = lm_datasets['train'].num_rows\n","if PARAMS['num_epochs'] == 1:\n","  warmup_steps = math.ceil(num_rows / PARAMS['batch_size']) // 2\n","  save_steps = warmup_steps * 2\n","  eval_steps = (save_steps * 5.0) // 100\n","else:\n","  warmup_steps = math.ceil(num_rows / PARAMS['batch_size'])\n","  save_steps = (warmup_steps * PARAMS['num_epochs']) // 2\n","  eval_steps = (warmup_steps * PARAMS['num_epochs'] * 5.0) // 100\n","\n","print(\"\\n\")\n","print(\"Linear Warm Up: \", warmup_steps)\n","print(\"Save Steps: \", save_steps)\n","print(\"Eval Steps: \", eval_steps)\n","\n","training_args = TrainingArguments(\n","    output_dir = PARAMS['model_output'],\n","    evaluation_strategy = 'steps',\n","    eval_steps = eval_steps,\n","    logging_steps = eval_steps,\n","    save_steps = save_steps,\n","    save_total_limit = 1,\n","    warmup_steps = warmup_steps,\n","    learning_rate = PARAMS['lr'],\n","    per_device_train_batch_size = PARAMS['batch_size'],\n","    num_train_epochs = PARAMS['num_epochs'],\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=lm_datasets[\"train\"],\n","    eval_dataset=lm_datasets[\"test\"],\n",")\n","\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vx5e_LrWcm4E"},"source":[""],"execution_count":null,"outputs":[]}]}